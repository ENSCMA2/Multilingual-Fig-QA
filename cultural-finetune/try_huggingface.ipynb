{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://huggingface.co/docs/transformers/en/training\n",
    "- https://huggingface.co/docs/transformers/en/peft\n",
    "- https://huggingface.co/docs/peft/quicktour\n",
    "- https://jaotheboss.medium.com/peft-with-bert-8763d8b8a4ca\n",
    "- https://huggingface.co/learn/nlp-course/en/chapter7/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    AutoModel,\n",
    "    XLMRobertaTokenizer,\n",
    "    XLMRobertaXLModel,\n",
    "    AutoModelForMaskedLM,\n",
    "    XLMRobertaXLConfig,\n",
    "    XLMRobertaXLForMultipleChoice\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_CODE = 'yo'\n",
    "DSETSIZE = 10000\n",
    "SCORER = 'bm25'\n",
    "NUM_EXAMPLES = 100\n",
    "dataset = load_from_disk(f\"../culturaldataset/select_datasets/{LANG_CODE}/{SCORER}-{DSETSIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.781022583199985,\n",
       " 'example': 'AfonrereYORUBA Gbode\\nE kaabo si Afonrere YORUBA\\nIwe Irohin fun imugbooro, ilosiwaju ati idagbasoke Yoruba\\nÌwé Ìròhìn fún Àmúgbòòrò, Ìlọsíwájú àti Ìdàgbàsókè Yorùbá\\nẸ káábọ̀, ẹ̀yin alárá wa. Ẹ̀yà Yorùbá wà káàkiri gbogbo àgbáyé. Wọ́n pọ̀, wọ́n gbọ́n, wọ́n ní òye, àsà, ọ̀làjú, ètò bí a se ń se Ìlú àti àkóso. Bẹ́ẹ̀ni wọ́n sì jáfáfá. A kò se iyèméjì pé bí ẹ̀yà Yorùbá bá rí ọwọ́ mú lágbàńlá-ayé, Aláwọ̀dúdú rí ọwọ́ mú nìyan. Ìdà kejì ọ̀rọ̀ yìi rí bẹ́ẹ̀, àmọ́ a ò gbàdúrà rẹ̀.\\nÀdúrà nìkan kò tó sá. A gbọdọ̀ sisé tọ̀ ọ́ ni. Èyí ló gbún wa ní kẹ́sẹ́ láti dá Ìwé Ìròhìn Afọnrere YORÙBÁ ỳií sílẹ̀. Èdè Yorùbá ni a ó maa fi kọ ọ́. A ó ma tu díẹ̀ nínù Ìròhìn wa sí èdè Gẹ̀ẹ́sì, èdè Faransé àti Śpáníisì ní Ìdákọ̀ọ̀kan.\\nKí ẹ máa bá wa kálọ.\\nẸ fi Ìwé yín sọwọ́ sí wa ní afonrereyoruba@yahoo.com\\nJÀRE ÀJÀYÍ\\nOlùdásílẹ̀ tí í tún se Olóòtú Àgbà\\nỌ̀pọ̀ nǹkan ni ẹ ó ma rí kà nínú Ìwé Ìròhìn yí lóòrèkóòrè.\\nAfọnrere ẁa fún gbogbo m̀ut́uḿuẁa.\\nẸ wo ̀Ìtọ́ka tó wà lókè láti fi ka ọ̀kan-ò-jọ̀kan ohun ti a tẹ̀ sínú AfọnrereYorùbá. Àkà gbádùn ni.\\nẸ o rí Ìròhìn káàkiri àgbáyé tó jẹ mọ́ Yorùbá nínú rẹ̀. Ìròhìn láti Nàijíríà, Àmẹ́ríkà, ilẹ̀ Gẹ̀ẹ́ẹ́sì, Ìbáábá (the Diaspora),Ìwọ̀ Afrika, Ùrúgúayè, Brasiili, Kuba (Cuba) àti ibi gbogbo ní àgbáyé tí Yorùbá ti ń se bẹbẹ.\\nẸ máa kàá, kí ẹ sì máa kọ̀we yiń sí wa,\\nOlóòtú.\\nFún ̀ipolongo, ̀Ì̀ḿugb̀òor̀o ̀ati ̀ilọs̀iẃaj́u Yor̀ub́a.\\ǹIfara-ẹni ĺeti: A woye pe Apẹ-̀it̀ẹẃe (Keyboard) ti a l̀o ́n gb́e ǹnkan ̀aj̀eji j́ade ĺoŕI Kọmṕut̀a ̀awọn kan. Ńitori ̀eyi ni a se ̀Akọwọĺe yii ni ̀ẹ̀ẹmeji - ̀ọkan pẹlu Ape-itẹwe Ḱọnyin, ̀ekeji pẹĺu Apẹ-itẹwe (ati f́ọǹnti) miran\\nOĺòot́u.\\nE kaabo, eyin alara wa. Eya Yoruba wa kaakiri gbogbo agbaye. Won po, won gbon, won ni oye, asa, olaju, eto bi a se n se ilu ati akoso. Beeni won si jafafa. A ko se iyemeji pe bi eya Yoruba ba ri owo mu lagbanla-aye, Alawodudu ri owo mu niyan. Ida keji oro yii ri bee, amo a o gbadura re.\\nAdura nikan ko to sa. A gbodo sise to o ni. Eyi lo gbun wa ni kese lati da iwe irohin Afonrere YORUBA yii sile. Ede Yoruba ni a o maa fi ko o. A o ma tu die ninu irohin wa se ede Geesi, ede Faranse ati Spaniisi ni idakookan.\\nKi e maa ba wa kalo.\\nE fi iwe yin sowo siwa ni afonrereyoruba@yahoo.com\\nOludasile ti i tun se Olootu Agba.\\nOpo nnkan ni e o ri ma ri ka ninu iwe irohin yi loorekoore.\\nAfonrere wa fun gbogo mutumuwa.\\nE wo Itoka to wa loke lati fi ka okan-o-jokan ohun ti a te sinu AfonrereYoruba. Akagabadun ni. E o ri irohin kaakiri agbaye to je mo Yoruba ninu re. Irohin lati Naijiria, Amerika, ile Geesi, Ibaaba (Diaspora), Uruguaye, ati bee bee lo.\\nE maa kaa ki e si maa kowe yin si wa,\\nOlootu.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XLMRobertaTokenizerFast' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XLMRobertaTokenizerFast' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "model.to('cpu')\n",
    "tokenizer.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 402.35 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['score', 'example', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"example\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = dataset.select(range(NUM_EXAMPLES)).map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaModel(\n",
      "  (embeddings): XLMRobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): XLMRobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x XLMRobertaLayer(\n",
      "        (attention): XLMRobertaAttention(\n",
      "          (self): XLMRobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): XLMRobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): XLMRobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): XLMRobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): XLMRobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    # target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 278,633,472 || trainable%: 0.21168454592562375\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters() # see % trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): XLMRobertaModel(\n",
      "      (embeddings): XLMRobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): XLMRobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): XLMRobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['score', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.rename_column(\"example\", \"text\")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(tokenized_dataset['text'][0][:100], return_tensors=\"pt\")\n",
    "inputs = inputs.to('cpu')\n",
    "lora_model = lora_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0873,  0.1106,  0.0666,  ..., -0.0501,  0.0721, -0.0197],\n",
       "         [-0.0081, -0.1164,  0.0249,  ..., -0.0507,  0.0636,  0.0125],\n",
       "         [-0.0304,  0.1153,  0.0092,  ..., -0.0534, -0.0404,  0.1008],\n",
       "         ...,\n",
       "         [ 0.0829,  0.0443,  0.0210,  ..., -0.1380, -0.0108,  0.1600],\n",
       "         [ 0.0543,  0.0591,  0.0068,  ..., -0.1866,  0.0065, -0.0482],\n",
       "         [ 0.0727,  0.1022,  0.0085,  ..., -0.1361,  0.0007,  0.0193]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-3.0522e-02,  2.7087e-01,  1.1834e-01,  5.1281e-01,  4.8539e-03,\n",
       "          3.6024e-01,  4.2014e-01, -4.4383e-01,  1.6263e-01, -1.5847e-01,\n",
       "          1.3781e-01,  9.1416e-02,  3.8526e-01,  3.3253e-01, -2.0363e-01,\n",
       "         -1.7617e-01,  1.8961e-01,  4.2476e-01, -7.3580e-02, -2.2573e-01,\n",
       "         -2.4010e-01,  3.4361e-01, -6.7378e-01, -5.5283e-01, -2.1458e-01,\n",
       "          5.7239e-01,  1.3006e-01, -3.1152e-01, -1.3167e-01,  6.4258e-01,\n",
       "          1.2998e-01,  4.8169e-01, -2.7538e-01,  1.0443e-01,  1.1554e-01,\n",
       "         -1.8037e-01,  3.6609e-01,  2.5511e-01,  4.6981e-01,  3.2743e-01,\n",
       "          1.1203e-01, -1.4105e-01, -1.3315e-01,  2.5133e-01,  2.2549e-01,\n",
       "         -2.3061e-01,  1.8636e-01, -7.6193e-02, -2.5662e-01,  3.8425e-01,\n",
       "          5.6150e-01, -2.6576e-01,  5.6660e-02,  4.8738e-02,  1.8754e-01,\n",
       "          1.9427e-01,  3.0647e-01, -3.2028e-01, -2.4153e-01, -4.9896e-01,\n",
       "          5.9186e-03,  6.1895e-01,  4.2733e-01,  3.2951e-01,  2.9599e-01,\n",
       "         -5.5467e-01,  1.1601e-01, -5.1938e-02, -6.2070e-01,  1.9053e-01,\n",
       "          4.1959e-02, -3.3570e-01,  3.5969e-01,  6.5642e-02,  1.3232e-01,\n",
       "          4.8275e-02,  4.0664e-01,  1.5840e-01,  3.6095e-01,  2.7387e-01,\n",
       "          2.9502e-01, -3.7170e-01,  1.2798e-01, -2.1816e-01,  8.9681e-02,\n",
       "          6.6563e-01, -4.2723e-01,  7.5639e-01,  3.2957e-01,  2.9857e-01,\n",
       "         -1.8859e-01, -3.1807e-02, -3.0279e-01,  2.9447e-01, -3.9545e-01,\n",
       "          3.7193e-01, -3.5057e-01, -5.7578e-01,  1.4054e-01, -1.4999e-02,\n",
       "         -2.7255e-01, -3.8871e-01,  2.1479e-01, -4.6350e-01, -1.0794e-01,\n",
       "         -7.3387e-01,  5.0049e-01,  4.3568e-01, -1.8179e-01,  3.8172e-01,\n",
       "          1.8087e-01,  1.3264e-01,  3.1112e-01, -6.6153e-01,  7.7775e-02,\n",
       "          3.5964e-01,  5.2704e-01,  1.5951e-01,  8.2387e-03, -2.7706e-01,\n",
       "         -2.6529e-01,  5.4000e-01, -3.5913e-01, -1.4042e-01,  3.5347e-01,\n",
       "          8.1368e-02, -3.5371e-01,  6.4676e-01,  3.3415e-01, -4.6145e-02,\n",
       "          4.7839e-01, -3.2458e-01,  1.6356e-01, -5.7857e-01, -1.5051e-02,\n",
       "         -2.5838e-01,  5.2882e-01, -3.9194e-01, -2.5435e-02, -5.4438e-01,\n",
       "          3.8572e-01, -9.2126e-02, -1.8086e-01,  4.5131e-01, -2.0297e-01,\n",
       "          4.9097e-01,  5.1909e-01,  1.2212e-01, -3.0085e-01,  1.6697e-02,\n",
       "          6.2243e-01, -2.9371e-01,  1.9250e-01, -1.7590e-01,  1.3165e-01,\n",
       "          5.0095e-01, -3.3100e-02,  2.1749e-01, -1.6978e-01,  1.6656e-01,\n",
       "         -1.3351e-01,  3.1247e-01, -1.9228e-01, -3.3897e-01, -4.2529e-01,\n",
       "          2.5287e-01,  1.5918e-01,  7.4973e-01, -1.4524e-01,  2.3856e-01,\n",
       "         -2.7878e-01,  3.8275e-02,  4.8238e-01, -1.1303e-01, -2.4681e-01,\n",
       "         -5.4718e-01, -1.3396e-01, -3.5985e-01,  3.5361e-01,  7.7963e-03,\n",
       "         -3.3137e-01, -2.6527e-01,  7.3020e-01,  3.0107e-01, -1.4536e-01,\n",
       "          4.6920e-02, -1.9781e-01, -3.7338e-01, -2.1185e-02, -6.1495e-02,\n",
       "         -3.9785e-01, -1.8566e-01,  2.9754e-01, -4.9244e-01, -1.3847e-01,\n",
       "         -3.4761e-01, -9.9804e-02, -1.7752e-01, -3.1859e-01,  5.2006e-01,\n",
       "          4.0291e-01,  7.1297e-02, -4.1836e-01, -1.4033e-01, -1.4939e-02,\n",
       "          3.2481e-02, -2.7131e-01,  3.9401e-01, -1.8367e-02,  1.0552e-01,\n",
       "          2.9824e-02,  3.0093e-01, -7.0086e-02, -6.6255e-01, -3.8093e-01,\n",
       "          2.2909e-01, -7.4856e-01,  3.7032e-01, -2.6418e-01,  3.0806e-01,\n",
       "         -4.9606e-01,  3.8672e-01, -7.9541e-02, -5.5023e-01,  2.7074e-01,\n",
       "          5.7874e-01,  5.8126e-01, -4.9372e-01,  2.1214e-01, -1.0008e-01,\n",
       "          6.7752e-01, -1.7649e-02, -2.5235e-01, -2.2088e-01, -8.7170e-02,\n",
       "         -5.0108e-01,  3.6432e-01,  3.5326e-01, -5.1542e-01,  4.6516e-01,\n",
       "         -2.5033e-01,  5.8928e-02,  3.1986e-01,  7.1179e-02, -1.2457e-02,\n",
       "         -4.8852e-01,  3.1665e-01, -4.7000e-01,  3.0420e-01, -2.1929e-02,\n",
       "         -7.4849e-01, -2.8518e-01,  7.6850e-02,  6.2971e-01, -3.5194e-01,\n",
       "         -4.0513e-01, -4.7044e-02,  4.2070e-01, -5.3932e-01, -1.9226e-01,\n",
       "          1.2132e-01,  4.7157e-02, -8.8709e-02, -5.9969e-01, -4.5988e-01,\n",
       "          8.0997e-02,  6.7135e-03,  1.9220e-01, -2.4485e-01, -1.4938e-01,\n",
       "          5.0513e-01, -5.5711e-03,  2.3959e-01,  6.5291e-02, -3.6341e-01,\n",
       "          2.3856e-02,  1.2372e-01, -5.6491e-01,  2.6416e-01,  8.1241e-02,\n",
       "          7.1513e-01, -3.5550e-01,  3.4493e-01, -1.7480e-01,  1.4046e-01,\n",
       "         -8.4191e-02,  3.8637e-01,  7.4050e-02, -2.1893e-01,  4.6716e-01,\n",
       "         -4.9511e-01, -1.6652e-01,  1.5827e-01,  1.2847e-01, -3.0931e-01,\n",
       "          2.4306e-01, -2.8436e-01, -6.9740e-01,  1.7604e-01, -4.7831e-02,\n",
       "         -7.6150e-01, -7.7718e-01, -2.8064e-01, -2.9658e-01,  5.1034e-01,\n",
       "          2.5069e-01, -5.3961e-01,  7.6783e-06, -1.5582e-01, -1.7241e-03,\n",
       "          4.9334e-01,  2.5792e-01,  1.9910e-01,  2.4509e-02, -5.1781e-01,\n",
       "          3.4291e-01, -5.3623e-01,  1.9505e-02, -1.2514e-01,  2.9924e-01,\n",
       "          1.6378e-01,  2.1787e-01,  7.1601e-02,  6.9157e-01,  1.4862e-02,\n",
       "         -1.8413e-01, -4.1887e-01, -1.1913e-01, -2.5401e-02,  1.0361e-01,\n",
       "          4.2939e-01, -1.6393e-01, -3.7783e-01, -3.1966e-01,  1.4840e-01,\n",
       "          3.8857e-01,  5.3480e-01,  2.5416e-01, -3.2854e-01, -6.2299e-03,\n",
       "         -8.9400e-02, -4.7628e-01, -5.7497e-01,  3.0791e-01, -5.1475e-01,\n",
       "         -3.9964e-01,  2.2340e-01,  2.6465e-02, -4.4242e-01, -9.3640e-02,\n",
       "         -4.9461e-01,  4.1391e-01, -1.1718e-02, -4.7622e-01, -1.1710e-01,\n",
       "          2.4241e-01,  5.3550e-01,  3.6032e-02,  2.4509e-01,  9.7648e-02,\n",
       "         -2.9295e-03,  3.2405e-02,  1.4537e-01,  4.7518e-02,  1.0251e-01,\n",
       "         -5.6492e-01, -6.9038e-02, -2.0927e-01,  5.9823e-01,  1.2363e-01,\n",
       "          8.5533e-02,  8.3109e-02,  1.9420e-01, -3.1410e-01,  2.9225e-01,\n",
       "         -5.0622e-02, -3.3348e-01,  4.1078e-01, -2.5168e-01, -2.0241e-01,\n",
       "          4.9960e-01,  1.6308e-01, -8.7913e-02, -4.9433e-03,  1.9168e-01,\n",
       "          3.4251e-01,  5.4962e-01, -7.6686e-02,  6.5425e-01, -1.0396e-01,\n",
       "          5.2159e-01,  1.1427e-01, -1.6788e-01, -2.7949e-01,  4.6422e-02,\n",
       "         -5.9928e-01,  5.4851e-01, -4.8764e-01,  3.5428e-01, -4.1172e-02,\n",
       "         -3.6411e-01, -4.1590e-01,  6.3511e-01,  4.3887e-01,  3.9945e-01,\n",
       "          1.9021e-01,  1.0429e-01,  5.2853e-01, -6.5144e-01,  2.7260e-01,\n",
       "          3.2295e-01,  1.2223e-01,  1.6178e-01, -3.9343e-01, -1.7335e-01,\n",
       "         -1.7469e-01,  3.0774e-03, -2.1566e-01, -2.8974e-01, -1.5379e-01,\n",
       "          3.0161e-01, -5.9242e-01,  2.3800e-01,  7.4885e-01,  6.4072e-01,\n",
       "         -1.4035e-01,  1.4531e-01, -8.6617e-02, -2.3842e-01, -6.7150e-01,\n",
       "          2.4087e-02, -1.1178e-01, -4.6919e-01,  1.3541e-01, -1.8720e-01,\n",
       "         -5.2219e-01,  1.6163e-02,  1.8279e-01, -3.1161e-02,  6.7918e-02,\n",
       "         -1.6067e-01,  1.8387e-01,  4.1118e-01, -2.4923e-01, -7.2579e-02,\n",
       "         -2.5008e-01,  1.3119e-01,  3.4377e-01, -1.3208e-01,  4.0922e-02,\n",
       "          2.3047e-01, -3.1872e-01,  5.5324e-01,  1.7444e-01,  6.2807e-01,\n",
       "          3.1972e-01,  1.4898e-03,  2.0418e-01,  1.0679e-01, -3.6687e-01,\n",
       "          3.0179e-01, -4.1041e-01, -1.0321e-01,  7.7127e-02, -4.4648e-01,\n",
       "          1.6101e-01, -2.8385e-01, -7.2120e-01,  2.9841e-01,  3.8227e-01,\n",
       "         -2.9139e-01, -4.3364e-01,  5.3648e-02,  5.4056e-02, -4.5396e-01,\n",
       "         -1.3738e-01,  2.1724e-01, -1.0072e-01,  7.5715e-02,  2.5966e-01,\n",
       "          2.2396e-01,  1.1694e-02, -3.3872e-01,  3.2294e-01, -3.6349e-01,\n",
       "          6.5431e-01, -5.7919e-02,  1.5387e-02, -1.3012e-01, -4.1638e-01,\n",
       "         -1.1309e-01,  5.3548e-02,  1.7443e-01,  6.5256e-01, -1.8019e-01,\n",
       "          2.6525e-01,  1.2983e-01, -3.5583e-01, -4.2642e-01, -5.9915e-01,\n",
       "         -1.3111e-01,  7.6598e-02, -3.4974e-01,  2.2329e-01,  4.0580e-01,\n",
       "         -1.2140e-01, -7.8338e-02, -1.1400e-01,  1.7326e-01,  5.6882e-02,\n",
       "          3.3305e-01,  7.1053e-02, -5.8404e-01, -3.2595e-01, -6.9936e-02,\n",
       "         -2.5185e-01,  5.1291e-01,  3.6417e-01,  6.6000e-02,  1.8161e-01,\n",
       "          5.5686e-01, -3.4979e-01, -7.8904e-03,  2.1895e-01, -1.5562e-01,\n",
       "         -1.7693e-01, -3.1938e-01,  1.2501e-02,  4.6930e-01,  1.5924e-01,\n",
       "         -3.6118e-01, -5.2881e-01,  3.0563e-01,  2.9137e-01, -3.1334e-01,\n",
       "         -2.4574e-01,  6.5552e-01,  1.8895e-01, -8.2376e-02,  1.8298e-01,\n",
       "          3.8118e-01,  3.6561e-02,  3.0723e-01,  2.3965e-01,  7.7771e-01,\n",
       "          1.3246e-01, -1.4400e-01,  2.4794e-01, -5.2039e-01, -3.6626e-01,\n",
       "         -3.3627e-01,  6.5885e-01,  1.2101e-01,  3.9063e-01,  1.1463e-01,\n",
       "          1.2431e-01,  1.9154e-01,  2.2972e-01, -3.1814e-01, -2.9923e-01,\n",
       "          1.4912e-01,  4.8327e-01, -1.5642e-02, -2.7944e-01, -2.3289e-01,\n",
       "          4.0414e-01,  1.4533e-01, -4.4922e-01, -4.1470e-01,  4.0649e-01,\n",
       "          5.4742e-02, -8.3261e-02, -2.6461e-01,  2.9243e-01,  2.1365e-01,\n",
       "          2.5998e-01, -4.1013e-01, -5.1365e-01,  2.9090e-01, -2.9745e-01,\n",
       "         -2.0940e-01,  7.5964e-03,  7.3809e-01,  1.6046e-01, -5.4697e-02,\n",
       "          4.0628e-02, -4.8728e-02, -4.4974e-01, -4.1561e-01,  1.3515e-01,\n",
       "         -7.3193e-01,  2.1114e-01,  5.0415e-01, -1.0773e-01, -2.3917e-01,\n",
       "         -2.8819e-01,  7.3274e-03, -4.4161e-01,  2.2966e-01, -1.3946e-01,\n",
       "         -4.2845e-01,  2.7748e-01, -2.9453e-01, -4.4684e-02, -3.2176e-01,\n",
       "         -1.7537e-01,  3.0997e-01,  7.1654e-01, -5.1098e-01, -2.8507e-01,\n",
       "         -5.7600e-01,  1.6333e-01, -1.1128e-01,  3.4868e-02, -6.2413e-01,\n",
       "         -6.9084e-01,  3.4048e-02,  1.0300e-01,  5.7634e-01, -1.5982e-01,\n",
       "          2.1259e-01,  5.0927e-02,  1.1163e-01, -2.8022e-01,  8.0464e-01,\n",
       "          4.5050e-01, -3.1520e-01, -3.5230e-01, -5.4669e-01, -2.4296e-01,\n",
       "         -3.4994e-01, -3.9763e-01, -1.2340e-01, -3.8703e-01, -5.7340e-02,\n",
       "         -1.2038e-01,  8.1911e-01, -2.8747e-01, -3.9564e-01, -2.5116e-02,\n",
       "         -4.6458e-02, -1.1081e-01,  7.5701e-02,  1.4007e-02, -2.0885e-01,\n",
       "         -2.2135e-01,  2.7505e-01,  7.5554e-02, -2.6855e-02, -3.3168e-01,\n",
       "         -3.1357e-01, -6.0235e-01,  6.8373e-02,  2.0056e-01, -2.1154e-01,\n",
       "         -4.4370e-01,  5.9375e-01,  1.2857e-01,  1.8038e-01, -4.7329e-01,\n",
       "          4.2654e-01, -3.8506e-01,  1.8007e-01, -2.1931e-01, -6.6307e-02,\n",
       "         -3.0325e-01, -2.4369e-01, -9.0474e-02,  1.6410e-01,  2.0684e-02,\n",
       "          4.1473e-01, -1.6005e-01,  4.4078e-01,  1.0403e-01,  1.4575e-02,\n",
       "          2.1401e-01,  4.9749e-01,  4.3478e-02, -6.3900e-01, -2.0372e-01,\n",
       "         -7.7016e-01,  1.0586e-01, -1.8601e-04,  4.3131e-01, -5.9513e-01,\n",
       "         -9.8380e-02,  4.1109e-01,  9.1139e-02, -2.0588e-01,  1.7616e-01,\n",
       "         -3.3594e-01,  6.4446e-01,  4.6281e-01,  3.0665e-01, -4.7896e-02,\n",
       "          3.7559e-01,  6.1120e-01,  8.7104e-02, -9.1278e-02, -6.2931e-01,\n",
       "          4.1829e-02,  2.5443e-01,  1.4426e-01, -1.2761e-01,  1.0332e-01,\n",
       "         -6.7046e-01, -3.4290e-01,  3.7866e-01, -5.1336e-01, -1.3931e-01,\n",
       "          3.4163e-01, -1.1638e-01,  2.8504e-01, -6.2217e-01, -3.9443e-01,\n",
       "         -3.8055e-01, -1.8056e-01,  2.3463e-01, -4.1524e-01,  8.8365e-02,\n",
       "         -4.6020e-01, -1.3590e-01, -1.8819e-02,  2.6873e-02, -2.7150e-01,\n",
       "         -2.1365e-01, -3.6985e-01,  2.5725e-01, -8.5989e-02, -1.8690e-01,\n",
       "          3.1070e-02, -1.0390e-01,  4.2430e-03, -2.9351e-01, -1.3776e-01,\n",
       "         -3.0568e-01, -3.7033e-01, -2.1061e-01, -7.9704e-02, -2.0380e-01,\n",
       "          3.3949e-01, -4.1567e-01, -7.6885e-03, -6.3437e-02,  1.4275e-01,\n",
       "         -3.1242e-01,  8.0970e-01,  4.7876e-02, -1.2050e-01, -5.0393e-01,\n",
       "          2.6336e-01, -3.3783e-02, -4.4076e-01, -5.6022e-02, -1.2290e-01,\n",
       "         -1.8923e-01,  3.6592e-01,  5.7422e-01,  5.9204e-01, -5.3449e-01,\n",
       "         -1.2848e-01,  6.3214e-01, -2.7381e-01,  4.8981e-01, -4.2329e-01,\n",
       "         -9.2393e-02, -1.0015e-01,  1.7286e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaosarium/anaconda3/envs/multi/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  0%|          | 0/7 [06:36<?, ?it/s]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XLMRobertaModel.forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mlora_model,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mTrainingArguments(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multi/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multi/lib/python3.10/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/multi/lib/python3.10/site-packages/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multi/lib/python3.10/site-packages/transformers/trainer.py:3059\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/multi/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multi/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multi/lib/python3.10/site-packages/peft/peft_model.py:1946\u001b[0m, in \u001b[0;36mPeftModelForFeatureExtraction.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1945\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1946\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1947\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1948\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1956\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multi/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multi/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multi/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: XLMRobertaModel.forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"test_trainer\", \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=16,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> DistilBERT number of parameters: 67M'\n",
      "'>>> BERT number of parameters: 110M'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great deal.'\n",
      "'>>> This is a great success.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great feat.'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.81k/7.81k [00:00<00:00, 15.7MB/s]\n",
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:01<00:00, 18.4MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 25.3MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:01<00:00, 32.1MB/s]\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 325430.46 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 386076.48 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 383714.98 examples/s]\n",
      "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 25000/25000 [00:05<00:00, 4874.58 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:05<00:00, 4939.38 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:13<00:00, 3753.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:41<00:00, 601.71 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:40<00:00, 612.41 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [01:24<00:00, 589.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61291\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59904\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 122957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] i rented [MASK] am curious - yellow from my video store because of all the [MASK] that surrounded it when it was first released in 1967. i also heard that at first it was seized by [MASK]. s. customs if [MASK] ever [MASK] [MASK] enter this country, therefore [MASK] a fan of films [MASK] \"ree [MASK] i really had to see this for myself. < br / > < br / [MASK] the [MASK] is centered around a young swedish [MASK] student named lena [MASK] [MASK] to learn everything she can about life. in particular she wants to [MASK] her attentions to making some sort of documentary on what the [MASK] sw [MASK] thought about certain political [MASK] such'\n",
      "\n",
      "'>>> as the vietnam war and race issues in the united states. in [MASK] asking [MASK] and ordinary denizens of stockholm about [MASK] opinions on politics, she [MASK] sex president her drama teacher, classmates, and married men. < br / > < [MASK] / > what kills me about i am curious - yellow is [MASK] 40orth ago [MASK] this was [MASK] pornographic. really, the sex and nudity scenes are few [MASK] far between, even [MASK] [MASK]'s not shot like some [MASK]ly [MASK] porno. while my countrymen mind find it shocking, in reality sex and nudity are a [MASK] staple in swedish cinema. physics ingmar bergman,'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
