{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from finetune import *\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import get_full_repo_name\n",
    "from huggingface_hub import Repository\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cultural_corpus': 'yo-bm25-10000',\n",
       " 'pretrained_model': 'FacebookAI/xlm-roberta-base',\n",
       " 'corpus_chunk_size': 128,\n",
       " 'wwm_probability': 0.15,\n",
       " 'batch_size': 64}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask token id is 250001 and mask token is <mask>\n",
      "tokenizer max len 512\n",
      "model config: XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForMultipleChoice were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_model num param: 278295186\n",
      "mc_model num param: 278044417\n"
     ]
    }
   ],
   "source": [
    "tokenizer = mk_tokenizer(toy_args)\n",
    "mlm_model, mc_model = mk_models(toy_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _:_.\n",
      "pred 1: The cat said _,_.\n",
      "pred 2: The cat said _that_.\n",
      "pred 3: The cat said _:_.\n",
      "pred 4: The cat said _._.\n",
      "pred 5: The cat said _it_.\n",
      "pred 6: The cat said _he_.\n",
      "pred 7: The cat said _\"_.\n",
      "pred 8: The cat said _to_.\n",
      "pred 9: The cat said _..._.\n"
     ]
    }
   ],
   "source": [
    "fill_mask(toy_args, mlm_model, tokenizer, f\"The cat said {tokenizer.decode(tokenizer.mask_token_id)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'hidden_states'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_text = f\"The cat said {tokenizer.decode(tokenizer.mask_token_id)}.\"\n",
    "toy_input = tokenizer(toy_text, return_tensors=\"pt\")\n",
    "mlm_model(**toy_input).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 378/378 [00:00<00:00, 30376.04 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 17704.60 examples/s]\n",
      "Map: 100%|██████████| 378/378 [00:00<00:00, 32407.65 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 13685.58 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 train example:\n",
      "{'input_ids': [0, 18826, 7, 5154, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 18826, 7, 5154, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 18826, 7, 5154, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 18826, 7, 5154, 163, 8770, 2, 0, 18826, 7, 5154, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [None, 0, 0, 1, 2, 2, None, None, 0, 1, 2, 3, 3, None, None, 0, 1, 2, 3, 3, None, None, 0, 1, 2, 3, 3, None, None, 0, 0, 0, 0, 1, 2, 3, 3, None, None, 0, 0, 1, 2, 2, None, None, 0, 0, 0, 0, 1, 2, 3, 3, None, None, 0, 1, 2, 3, 3, None, None, 0, 0, 1, 2, 2, None, None, 0, 0, 0, 0, 1, 2, 3, 3, None, None, 0, 0, 0, 0, 1, 2, 3, 3, None, None, 0, 0, 1, 2, 2, None, None, 0, 0, 1, 2, 2, None, None, 0, 0, 0, 0, 1, 2, 3, 3, None, None, 0, 1, 2, 3, 3, None, None, 0, 0, 0, 0, 1, 2, 3], 'labels': [0, 18826, 7, 5154, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 18826, 7, 5154, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 18826, 7, 5154, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 18826, 7, 5154, 163, 8770, 2, 0, 18826, 7, 5154, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163]}\n",
      "decoded: <s> Cats say meow</s><s> The cat said meow</s><s> The cat said meow</s><s> The cat said meow</s><s> Tokenizers are so meow</s><s> Cats say meow</s><s> Tokenizers are so meow</s><s> The cat said meow</s><s> Cats say meow</s><s> Tokenizers are so meow</s><s> Tokenizers are so meow</s><s> Cats say meow</s><s> Cats say meow</s><s> Tokenizers are so meow</s><s> The cat said meow</s><s> Tokenizers are so me\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 23\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_corpus, corpus_mask_collator, corpus_wwm_collator = mk_corpus(toy_args, tokenizer)\n",
    "lm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 20086.46 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 1612.11 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 763.16 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figqa data looks like:\n",
      "train: {'labels': 0, 'input_ids': [[0, 581, 7515, 2804, 163, 8770, 2, 2, 163, 8770, 2], [0, 581, 7515, 2804, 163, 8770, 2, 2, 4323, 4390, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "decoded: ['<s> The cat said meow</s></s> meow</s>', '<s> The cat said meow</s></s> woof</s>']\n",
      "val: {'labels': 0, 'input_ids': [[0, 581, 7515, 2804, 163, 8770, 2, 2, 163, 8770, 2], [0, 581, 7515, 2804, 163, 8770, 2, 2, 4323, 4390, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "decoded: ['<s> The cat said meow</s></s> meow</s>', '<s> The cat said meow</s></s> woof</s>']\n",
      "test: {'labels': 0, 'input_ids': [[0, 62, 45730, 7515, 7, 1884, 47, 3249, 83, 163, 8770, 2, 2, 163, 8770, 2], [0, 62, 45730, 7515, 7, 1884, 47, 3249, 83, 163, 8770, 2, 2, 4323, 4390, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "decoded: ['<s> A sound cats like to make is meow</s></s> meow</s>', '<s> A sound cats like to make is meow</s></s> woof</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figqa_datasets, figqa_data_collator = mk_figqa_dataset(toy_args, tokenizer)\n",
    "figqa_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaosarium/anaconda3/envs/multi/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"xlmr-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=toy_args['batch_size'],\n",
    "    per_device_eval_batch_size=toy_args['batch_size'],\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    use_cpu=True,\n",
    "    use_mps_device=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=mlm_model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_corpus[\"train\"],\n",
    "    eval_dataset=lm_corpus[\"val\"],\n",
    "    data_collator=corpus_mask_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 405.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 26.20921516418457,\n",
       " 'eval_runtime': 0.3717,\n",
       " 'eval_samples_per_second': 2.69,\n",
       " 'eval_steps_per_second': 2.69}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:11<01:47, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 24.9817, 'grad_norm': 804.8196411132812, 'learning_rate': 0.00045000000000000004, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 10%|█         | 1/10 [00:12<01:47, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.300202369689941, 'eval_runtime': 0.293, 'eval_samples_per_second': 3.413, 'eval_steps_per_second': 3.413, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:23<01:34, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.855, 'grad_norm': 95.8296890258789, 'learning_rate': 0.0004, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 20%|██        | 2/10 [00:24<01:34, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.265312194824219, 'eval_runtime': 0.2862, 'eval_samples_per_second': 3.494, 'eval_steps_per_second': 3.494, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:35<01:21, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.783, 'grad_norm': 35.669010162353516, 'learning_rate': 0.00035, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 30%|███       | 3/10 [00:35<01:21, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.873168468475342, 'eval_runtime': 0.2918, 'eval_samples_per_second': 3.427, 'eval_steps_per_second': 3.427, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:46<01:09, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1409, 'grad_norm': 57.0134162902832, 'learning_rate': 0.0003, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 40%|████      | 4/10 [00:46<01:09, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.003854751586914, 'eval_runtime': 0.2684, 'eval_samples_per_second': 3.726, 'eval_steps_per_second': 3.726, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:57<00:57, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.168, 'grad_norm': 66.0810775756836, 'learning_rate': 0.00025, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 50%|█████     | 5/10 [00:58<00:57, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.64418625831604, 'eval_runtime': 0.2772, 'eval_samples_per_second': 3.608, 'eval_steps_per_second': 3.608, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:09<00:45, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6576, 'grad_norm': 36.099918365478516, 'learning_rate': 0.0002, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 60%|██████    | 6/10 [01:09<00:45, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9957005977630615, 'eval_runtime': 0.2923, 'eval_samples_per_second': 3.421, 'eval_steps_per_second': 3.421, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:21<00:35, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1602, 'grad_norm': 32.6023063659668, 'learning_rate': 0.00015, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 70%|███████   | 7/10 [01:21<00:35, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0624873638153076, 'eval_runtime': 0.278, 'eval_samples_per_second': 3.597, 'eval_steps_per_second': 3.597, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:32<00:23, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2333, 'grad_norm': 33.87591552734375, 'learning_rate': 0.0001, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 80%|████████  | 8/10 [01:32<00:23, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7064595222473145, 'eval_runtime': 0.2804, 'eval_samples_per_second': 3.566, 'eval_steps_per_second': 3.566, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:43<00:11, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8893, 'grad_norm': 21.227737426757812, 'learning_rate': 5e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 90%|█████████ | 9/10 [01:43<00:11, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.72381591796875, 'eval_runtime': 0.2943, 'eval_samples_per_second': 3.398, 'eval_steps_per_second': 3.398, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:54<00:00, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9403, 'grad_norm': 22.08307647705078, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 10/10 [01:54<00:00, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.725783586502075, 'eval_runtime': 0.283, 'eval_samples_per_second': 3.534, 'eval_steps_per_second': 3.534, 'epoch': 10.0}\n",
      "{'train_runtime': 114.9341, 'train_samples_per_second': 1.392, 'train_steps_per_second': 0.087, 'train_loss': 6.580933213233948, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=6.580933213233948, metrics={'train_runtime': 114.9341, 'train_samples_per_second': 1.392, 'train_steps_per_second': 0.087, 'train_loss': 6.580933213233948, 'epoch': 10.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _me_.\n",
      "pred 1: The cat said _ow_.\n",
      "pred 2: The cat said _ken_.\n",
      "pred 3: The cat said _s_.\n",
      "pred 4: The cat said _Cat_.\n",
      "pred 5: The cat said _say_.\n",
      "pred 6: The cat said _said_.\n",
      "pred 7: The cat said _are_.\n",
      "pred 8: The cat said _To_.\n",
      "pred 9: The cat said _The_.\n"
     ]
    }
   ],
   "source": [
    "fill_mask(toy_args, mlm_model, tokenizer, f\"The cat said {tokenizer.decode(tokenizer.mask_token_id)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM but with train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_word_ids_lm_corpus = lm_corpus.remove_columns([\"word_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 42.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 581, 7515, 2804, 250001, 8770, 2, 0, 250001, 7515, 187318, 163, 8770, 2, 0, 18826, 7, 5154, 163, 8770, 2, 0, 18826, 7, 5154, 163, 250001, 2, 0, 18826, 7, 5154, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0, 18826, 7, 250001, 250001, 8770, 2, 0, 250001, 7515, 2804, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 38748, 8770, 2, 0, 18826, 250001, 5154, 250001, 8770, 2, 0, 717, 1098, 250001, 7, 621, 221, 163, 250001, 2, 0, 581, 7515, 250001, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 221, 163, 8770, 2, 0, 717, 1098, 52825, 250001, 621, 221, 163, 8770, 2, 0, 717, 1098, 52825, 7, 621, 250001, 163, 8770, 2, 0, 581, 7515, 2804, 163, 8770, 2, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, 163, -100, -100, -100, 581, -100, 2804, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8770, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5154, 163, -100, -100, -100, 581, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 163, -100, -100, -100, -100, 7, -100, 163, -100, -100, -100, -100, -100, 52825, -100, -100, -100, -100, 8770, -100, -100, -100, -100, 2804, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 7, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 221, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = corpus_mask_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "eval_dataset = no_word_ids_lm_corpus[\"val\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=no_word_ids_lm_corpus[\"val\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")\n",
    "print(eval_dataset[0])\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    no_word_ids_lm_corpus[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=toy_args['batch_size'],\n",
    "    collate_fn=corpus_mask_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=toy_args['batch_size'], collate_fn=default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(mlm_model.parameters(), lr=5e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=3,\n",
    "    num_training_steps=30,\n",
    ")\n",
    "# model_name = \"testtest\"\n",
    "# repo_name = get_full_repo_name(model_name)\n",
    "# repo_name\n",
    "# output_dir = model_name\n",
    "# repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cpu'), device(type='cpu'))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator1 = Accelerator(cpu=False)\n",
    "mlm_model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator1.prepare(mlm_model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n",
    "mlm_model.device, train_dataloader.device, eval_dataloader.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:21<03:16, 21.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 60827416741.042984\n",
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _:_.\n",
      "pred 1: The cat said _,_.\n",
      "pred 2: The cat said _that_.\n",
      "pred 3: The cat said _:_.\n",
      "pred 4: The cat said _._.\n",
      "pred 5: The cat said _it_.\n",
      "pred 6: The cat said _he_.\n",
      "pred 7: The cat said _\"_.\n",
      "pred 8: The cat said _to_.\n",
      "pred 9: The cat said _..._.\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:42<02:50, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 1: Perplexity: 455351.87918402714\n",
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _,_.\n",
      "pred 1: The cat said _:_.\n",
      "pred 2: The cat said _._.\n",
      "pred 3: The cat said _that_.\n",
      "pred 4: The cat said _to_.\n",
      "pred 5: The cat said _he_.\n",
      "pred 6: The cat said _it_.\n",
      "pred 7: The cat said _:_.\n",
      "pred 8: The cat said _you_.\n",
      "pred 9: The cat said _\"_.\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:04<02:31, 21.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 2: Perplexity: 267.6746430240227\n",
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _,_.\n",
      "pred 1: The cat said _._.\n",
      "pred 2: The cat said _that_.\n",
      "pred 3: The cat said _to_.\n",
      "pred 4: The cat said _me_.\n",
      "pred 5: The cat said _:_.\n",
      "pred 6: The cat said _a_.\n",
      "pred 7: The cat said _you_.\n",
      "pred 8: The cat said _the_.\n",
      "pred 9: The cat said _it_.\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:23<00:00, 23.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:28<02:14, 22.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 3: Perplexity: 133.7124227374177\n",
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _s_.\n",
      "pred 1: The cat said _ow_.\n",
      "pred 2: The cat said _me_.\n",
      "pred 3: The cat said _to_.\n",
      "pred 4: The cat said _._.\n",
      "pred 5: The cat said _the_.\n",
      "pred 6: The cat said _so_.\n",
      "pred 7: The cat said _,_.\n",
      "pred 8: The cat said _are_.\n",
      "pred 9: The cat said _that_.\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:50<01:50, 22.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 4: Perplexity: 44.57705552005966\n",
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _me_.\n",
      "pred 1: The cat said _s_.\n",
      "pred 2: The cat said _ow_.\n",
      "pred 3: The cat said _cat_.\n",
      "pred 4: The cat said _so_.\n",
      "pred 5: The cat said _said_.\n",
      "pred 6: The cat said _to_.\n",
      "pred 7: The cat said _are_.\n",
      "pred 8: The cat said _say_.\n",
      "pred 9: The cat said _Me_.\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:26<00:00, 26.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:18<01:36, 24.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 5: Perplexity: 42.654064490981256\n",
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _ow_.\n",
      "pred 1: The cat said _cat_.\n",
      "pred 2: The cat said _s_.\n",
      "pred 3: The cat said _Cat_.\n",
      "pred 4: The cat said _say_.\n",
      "pred 5: The cat said _me_.\n",
      "pred 6: The cat said _so_.\n",
      "pred 7: The cat said _said_.\n",
      "pred 8: The cat said _are_.\n",
      "pred 9: The cat said _ken_.\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:26<00:00, 26.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:45<01:15, 25.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 6: Perplexity: 47.98199169620135\n",
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _say_.\n",
      "pred 1: The cat said _ken_.\n",
      "pred 2: The cat said _s_.\n",
      "pred 3: The cat said _cat_.\n",
      "pred 4: The cat said _Cat_.\n",
      "pred 5: The cat said _said_.\n",
      "pred 6: The cat said _izer_.\n",
      "pred 7: The cat said _so_.\n",
      "pred 8: The cat said _To_.\n",
      "pred 9: The cat said _are_.\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:26<00:00, 26.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [03:12<00:51, 25.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 7: Perplexity: 23.768466292552215\n",
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _izer_.\n",
      "pred 1: The cat said _s_.\n",
      "pred 2: The cat said _me_.\n",
      "pred 3: The cat said _so_.\n",
      "pred 4: The cat said _ken_.\n",
      "pred 5: The cat said _cat_.\n",
      "pred 6: The cat said _Cat_.\n",
      "pred 7: The cat said _To_.\n",
      "pred 8: The cat said _said_.\n",
      "pred 9: The cat said _are_.\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:22<00:00, 22.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:35<00:24, 24.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 8: Perplexity: 19.470916677189024\n",
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _izer_.\n",
      "pred 1: The cat said _me_.\n",
      "pred 2: The cat said _s_.\n",
      "pred 3: The cat said _so_.\n",
      "pred 4: The cat said _ken_.\n",
      "pred 5: The cat said _cat_.\n",
      "pred 6: The cat said _ow_.\n",
      "pred 7: The cat said _said_.\n",
      "pred 8: The cat said _To_.\n",
      "pred 9: The cat said _are_.\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:23<00:00, 23.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:00<00:00, 24.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 9: Perplexity: 18.556267452002295\n",
      "input: The cat said <mask>.\n",
      "pred 0: The cat said _izer_.\n",
      "pred 1: The cat said _me_.\n",
      "pred 2: The cat said _so_.\n",
      "pred 3: The cat said _ow_.\n",
      "pred 4: The cat said _s_.\n",
      "pred 5: The cat said _ken_.\n",
      "pred 6: The cat said _cat_.\n",
      "pred 7: The cat said _Cat_.\n",
      "pred 8: The cat said _To_.\n",
      "pred 9: The cat said _say_.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(10)):\n",
    "    print('training...')\n",
    "    mlm_model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        outputs = mlm_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator1.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        # progress_bar.update(1)\n",
    "\n",
    "    print('evaluating...')\n",
    "    mlm_model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = mlm_model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator1.gather(loss.repeat(toy_args['batch_size'])))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try: perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError: perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "    fill_mask(toy_args, mlm_model, tokenizer, f\"The cat said {tokenizer.decode(tokenizer.mask_token_id)}.\")\n",
    "    # Save and upload\n",
    "    # accelerator.wait_for_everyone()\n",
    "    # unwrapped_model = accelerator.unwrap_model(model)\n",
    "    # unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    # if accelerator.is_main_process:\n",
    "    #     tokenizer.save_pretrained(output_dir)\n",
    "    #     repo.push_to_hub(\n",
    "    #         commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FigQA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = figqa_datasets[\"train\"]\n",
    "eval_dataset = figqa_datasets[\"val\"]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=figqa_data_collator, batch_size=toy_args['batch_size'])\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=figqa_data_collator, batch_size=toy_args['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(mc_model.parameters(), lr=2e-5)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=3,\n",
    "    num_training_steps=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(mc_model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   0,  581, 7515, 2804,  163, 8770,    2,    2,  163, 8770,    2],\n",
       "         [   0,  581, 7515, 2804,  163, 8770,    2,    2, 4323, 4390,    2]]),\n",
       " '<s> The cat said meow</s></s> meow</s>')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "batch['input_ids'][0], tokenizer.decode(batch['input_ids'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:04<00:04,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6984, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7041, grad_fn=<NllLossBackward0>)\n",
      "epoch 0: {'accuracy': 0.75}\n",
      "{'accuracy': {'accuracy': 0.75}, 'train_loss': 0.3520698547363281, 'epoch': 0, 'step': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6714, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6595, grad_fn=<NllLossBackward0>)\n",
      "epoch 1: {'accuracy': 1.0}\n",
      "{'accuracy': {'accuracy': 1.0}, 'train_loss': 0.3297400176525116, 'epoch': 1, 'step': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:02<00:02,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6770, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6631, grad_fn=<NllLossBackward0>)\n",
      "epoch 2: {'accuracy': 0.75}\n",
      "{'accuracy': {'accuracy': 0.75}, 'train_loss': 0.3315522372722626, 'epoch': 2, 'step': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:02<00:02,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6407, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6217, grad_fn=<NllLossBackward0>)\n",
      "epoch 3: {'accuracy': 0.75}\n",
      "{'accuracy': {'accuracy': 0.75}, 'train_loss': 0.31084996461868286, 'epoch': 3, 'step': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6012, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6833, grad_fn=<NllLossBackward0>)\n",
      "epoch 4: {'accuracy': 0.75}\n",
      "{'accuracy': {'accuracy': 0.75}, 'train_loss': 0.341672420501709, 'epoch': 4, 'step': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6031, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5491, grad_fn=<NllLossBackward0>)\n",
      "epoch 5: {'accuracy': 0.75}\n",
      "{'accuracy': {'accuracy': 0.75}, 'train_loss': 0.27456343173980713, 'epoch': 5, 'step': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5815, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5767, grad_fn=<NllLossBackward0>)\n",
      "epoch 6: {'accuracy': 0.75}\n",
      "{'accuracy': {'accuracy': 0.75}, 'train_loss': 0.28835242986679077, 'epoch': 6, 'step': 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:04<00:04,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5420, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4833, grad_fn=<NllLossBackward0>)\n",
      "epoch 7: {'accuracy': 0.75}\n",
      "{'accuracy': {'accuracy': 0.75}, 'train_loss': 0.24167154729366302, 'epoch': 7, 'step': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5684, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5956, grad_fn=<NllLossBackward0>)\n",
      "epoch 8: {'accuracy': 1.0}\n",
      "{'accuracy': {'accuracy': 1.0}, 'train_loss': 0.29778701066970825, 'epoch': 8, 'step': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5493, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5318, grad_fn=<NllLossBackward0>)\n",
      "epoch 9: {'accuracy': 1.0}\n",
      "{'accuracy': {'accuracy': 1.0}, 'train_loss': 0.265889436006546, 'epoch': 9, 'step': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_steps = 0\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    mc_model.train()\n",
    "    mc_model, total_loss, completed_steps = train_model(train_dataloader, mc_model, accelerator, optimizer, lr_scheduler, toy_args, completed_steps, checkpointing_steps=500)\n",
    "    eval_metric = eval_model(mc_model, eval_dataloader, metric, accelerator, epoch, toy_args)\n",
    "\n",
    "    print({\n",
    "        \"accuracy\": eval_metric,\n",
    "        \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "        \"epoch\": epoch,\n",
    "        \"step\": completed_steps,\n",
    "    })\n",
    "\n",
    "    if False:\n",
    "        if toy_args[\"push_to_hub\"] and epoch < toy_args[\"num_train_epochs\"] - 1:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(mc_model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                toy_args[\"output_dir\"], is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "            )\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(toy_args[\"output_dir\"])\n",
    "                # repo.push_to_hub(\n",
    "                #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False, auto_lfs_prune=True\n",
    "                # )\n",
    "\n",
    "        if toy_args[\"checkpointing_steps\"] == \"epoch\":\n",
    "            output_dir = f\"epoch_{epoch}\"\n",
    "            if toy_args[\"output_dir\"] is not None:\n",
    "                output_dir = os.path.join(toy_args[\"output_dir\"], output_dir)\n",
    "            accelerator.save_state(output_dir)\n",
    "\n",
    "if False:\n",
    "    if toy_args[\"output_dir\"] is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(mc_model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            toy_args[\"output_dir\"], is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "        )\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(toy_args[\"output_dir\"])\n",
    "            # if args[\"push_to_hub\"]:\n",
    "            #     repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n",
    "        with open(os.path.join(toy_args[\"output_dir\"], \"all_results.json\"), \"w\") as f:\n",
    "            json.dump({\"eval_accuracy\": eval_metric[\"accuracy\"]}, f)\n",
    "\n",
    "eval_metric[\"accuracy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
